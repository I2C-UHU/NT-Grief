{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Aumentation for NT-Grief\n",
        "> This notebook focuses on utilizing the Back Translation technique to augment the minority class in sentiment datasets, specifically tailored for the NTgrief dataset. It creates an augmented file in English by first translating the minority class to German and then back to English. With appropriate adjustments, this technique can be similarly applied to the Spanish dataset (by translating to German and then back to Spanish)."
      ],
      "metadata": {
        "id": "VOqPSm-D5MFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
      ],
      "metadata": {
        "id": "BnEpLadj4wsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "file_path = 'path_to_file_here'\n",
        "df = pd.read_csv(file_path, sep='\\t', usecols=[\"id\", \"tweet\", \"label\"])"
      ],
      "metadata": {
        "id": "LRmIieRO4zh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Report the number of sentences\n",
        "print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display the class distribution\n",
        "count_class_0, count_class_1 = df['label'].value_counts()\n",
        "print(count_class_0, count_class_1)"
      ],
      "metadata": {
        "id": "zffo0drH4zNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the classes\n",
        "df_class_0 = df[df['label'] == 0]\n",
        "df_class_1 = df[df[\"label\"] == 1]\n"
      ],
      "metadata": {
        "id": "mhyiPQuk4y1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter for class 1 and convert text to lowercase\n",
        "df = df[df['label'] == 1]\n",
        "df['tweet'] = df['tweet'].str.lower()"
      ],
      "metadata": {
        "id": "XGLXj1xc4ygU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizers and translation models\n",
        "tokenizer_en_de = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\", use_fast=False)\n",
        "model_en_de = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n",
        "\n",
        "tokenizer_de_en = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-de-en\", use_fast=False)\n",
        "model_de_en = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-de-en\")"
      ],
      "metadata": {
        "id": "wbFO4esP4yKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an auxiliary dataframe\n",
        "df_aux = pd.DataFrame(columns=['id', 'tweet', 'label'])\n",
        "aux = []"
      ],
      "metadata": {
        "id": "U6tFA4Dz4-Na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform back-translation\n",
        "for f in df['tweet']:\n",
        "    original_text = f\n",
        "    print(\"Original: \", original_text)\n",
        "\n",
        "    # Translate from English to German\n",
        "    tokenized_text_en_de = tokenizer_en_de.prepare_seq2seq_batch(original_text, return_tensors='pt')\n",
        "    translation_en_de = model_en_de.generate(**tokenized_text_en_de)\n",
        "    translated_text_en_de = tokenizer_en_de.batch_decode(translation_en_de, skip_special_tokens=True)[0]\n",
        "\n",
        "    # Translate back from German to English\n",
        "    tokenized_text_de_en = tokenizer_de_en.prepare_seq2seq_batch(translated_text_en_de, return_tensors='pt')\n",
        "    translation_de_en = model_de_en.generate(**tokenized_text_de_en)\n",
        "    translated_text_de_en = tokenizer_de_en.batch_decode(translation_de_en, skip_special_tokens=True)[0]\n",
        "\n",
        "    print(\"Conversion: \", translated_text_de_en)\n",
        "\n",
        "    # Append the translated text to the auxiliary dataframe\n",
        "    aux.append(translated_text_de_en)"
      ],
      "metadata": {
        "id": "o7QbKJGG4-Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the auxiliary dataframe\n",
        "df_aux['id'] = df['id']\n",
        "df_aux['tweet'] = aux\n",
        "df_aux['label'] = df['label']"
      ],
      "metadata": {
        "id": "sa_ks4NK4-eZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate all dataframes to get the final augmented dataset\n",
        "df_final = pd.concat([df_aux, df_class_0], axis=0)\n",
        "df_final = pd.concat([df_final, df_class_1], axis=0)"
      ],
      "metadata": {
        "id": "dMGYAJVW4-mB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the augmented dataset\n",
        "output_file_path = 'path_to_train_output_file_here'\n",
        "df_final.to_csv(output_file_path, index=False, header=True, sep='\\t')\n"
      ],
      "metadata": {
        "id": "mWKUvyLw4uT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display class distribution in the augmented dataset\n",
        "count_class_0_aug, count_class_1_aug = df_final['label'].value_counts()\n",
        "print(count_class_0_aug, count_class_1_aug)"
      ],
      "metadata": {
        "id": "3Qosw7hk5JwN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}